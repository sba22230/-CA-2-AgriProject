{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Setup\n",
    "## Install neccessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas \n",
    "%pip install numpy  \n",
    "%pip install ipywidgets  \n",
    "%pip install matplotlib \n",
    "%pip install seaborn  \n",
    "%pip install sklearn \n",
    "%pip install pyarrow\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request as rq\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa # this is needed for the parquet file\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import interact, interactive, fixed, VBox\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE CHANGED\n",
    "# Function to load the Dublin bus gz files\n",
    "def load_Files(direc, files, comtype):\n",
    "    columns = ['Timestamp', 'LineID', 'Direction', 'JourneyPatternID', 'TimeFrame', 'VehicleJourneyID', 'Operator', 'Congestion', 'LonWGS84', 'LatWGS84', 'Delay', 'BlockID', 'VehicleID', 'StopID', 'AtStop']\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        yield pd.read_csv(direc + f, compression=comtype, delimiter=',', header=0, names=columns, parse_dates=True, low_memory=True)\n",
    "\n",
    "def crackit_open(busFile):\n",
    "    import zipfile as zip\n",
    "    # Zip creates its own folders - no need to check for folder existence\n",
    "    with zip.ZipFile(busFile,  mode='r') as arc: \n",
    "        arc.extractall('./Data/Bus/Gz/')  \n",
    "    files = os.listdir('./Data/Bus/Gz/')\n",
    "    DBfiles = [f for f in files if f.endswith('.gz')]\n",
    "    df = pd.concat(load_Files('./Data/Bus/Gz/', DBfiles, 'gzip'), copy = False)\n",
    "    return df\n",
    "\n",
    "def shapiro_test(x):\n",
    "    p_val = stats.shapiro(x)[1]\n",
    "    status = 'passed'\n",
    "    color = 'blue'\n",
    "    if p_val < 0.05:\n",
    "        status = 'failed'\n",
    "        color = 'red'\n",
    "    return status, color, p_val\n",
    "\n",
    "## TO BE CHANGED\n",
    "def custom_scatterplot(df1, col1=''):\n",
    "    df1 = df1[df1[\"LineID\"]==col1]\n",
    "    f = plt.figure()\n",
    "    f, ax = plt.subplots(figsize=(11.5, 11.5))\n",
    "    ax = f.add_subplot(projection='3d')\n",
    "    ax.scatter(df1['LonWGS84'], df1['LatWGS84'], df1['Hour'], alpha=0.6, color=df1['Colour'])\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_zlabel('Hour')\n",
    "    dcol = str(col1)\n",
    "    plt.savefig('./Images/Img_' + dcol + '_Longitude_Latitude_Hour.svg')\n",
    "    df1.to_parquet('./Data/Bus/LineID_' + dcol + '.parquet')\n",
    "    \n",
    "    \n",
    "def custom_barplot(df1, col1=''):\n",
    "    if len(df1[col1]) > 5000: # added this to the function because of warnings about the size of data being used with shapiro test\n",
    "            sampleSize = 5000\n",
    "    else:\n",
    "        sampleSize = len(df1[col1])\n",
    "    df1 = df1.sample(sampleSize) #shapiro test is unreliable over 5000 https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test and performance reasons\n",
    "    f, ax = plt.subplots(2,2, figsize=(11.5, 11.5))\n",
    "    ax = ax.reshape(-1)\n",
    "    df1[col1].plot(ax=ax[0], kind='hist')\n",
    "    ax[0].set_title('Histogram of {}'.format(col1))\n",
    "    df1[col1].plot(ax=ax[1], kind='kde')\n",
    "    ax[1].set_title('Density Plot of {}'.format(col1))\n",
    "    ax3 = plt.subplot(223)\n",
    "    stats.probplot(df[col1], plot=plt)\n",
    "    ax[2].set_title('QQ Plot of {}'.format(col1))\n",
    "    df1[col1].plot(ax=ax[3], kind='box')\n",
    "    ax[3].set_title('Box Plot of {}'.format(col1))\n",
    "    status, color, p_val = shapiro_test(df1[col1]) \n",
    "    f.suptitle('Normality test for {} {} (p_value = {})'.format(col1, status, p_val), color=color, fontsize=12)\n",
    "\n",
    "def num_missing(x):\n",
    "    return len(x.index)-x.count()\n",
    "\n",
    "def num_unique(x):\n",
    "    return len(np.unique(x))\n",
    "\n",
    "## TO BE CHANGED \n",
    "def load_csv_Files(direc, files):\n",
    "\n",
    "    for f in files:\n",
    "        # need to get number of rows to skip \n",
    "        temp=pd.read_csv(direc + f,sep='^',header=None,prefix='X')\n",
    "        temp2=temp.X0.str.split(',',expand=True)\n",
    "        del temp['X0']\n",
    "        temp=pd.concat([temp,temp2],axis=1)\n",
    "        cols = list(range(0,temp.shape[1]))\n",
    "\n",
    "        print(f)\n",
    "        yield pd.read_csv(direc + f,  delimiter=',', header=0,  parse_dates=True, low_memory=True, skiprows=14, usecols=cols, na_values='NAN')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure that the folder structure is present in the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./Data'):\n",
    "    print('Data folder exists')\n",
    "if os.path.exists('./Images/'):\n",
    "    print('Images folder exists')\n",
    "else:\n",
    "    os.makedirs('./Images/')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report TITLE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Can python read PDF for sentiment anaylysis?\n",
    "  + the answer to this is that yes Python has various packages that can extract text from pdf files, but none are particularly good or easy to use.\n",
    "+ Are free range pigs more profitable than indoor pigs?\n",
    "+ How does silage quality affect animal prices?\n",
    "  +  Further to the above how does the price of animal feeds affect the price of animals? \n",
    "    + but how do I incorporate sentiment analysis in to this question?\n",
    "+ Does dispossable income of households influence food prices?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP-DM Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Business Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial look at the data, check for missing data\n",
    "Discuss in detail the process of acquiring your raw data, detailing the positive and/or negative aspects of your research and acquisition. This should include the relevance and implications of any and all licensing/permissions associated with the data. [0-15]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory data analysis\n",
    "Exploratory Data Analysis helps to identify patterns, inconsistencies, anomalies, missing data, and other attributes and issues in data sets so problems can be addressed. \n",
    "\n",
    "Evaluate your raw data and detail, in depth, the various attributes and issues that you find. Your evaluation should reference evidence to support your  chosen methodology and use visualizations to illustrate your findings.[0-25]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the shape and types of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the numerical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Numerical and Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display plots from the Data set, including Histograms, pairplots, boxplots and correlation\n",
    "* possibly use seaborn reg plots to check linear regression from the correlation plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distributions of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Preparation\n",
    "\n",
    "All data frame changes go in this section, including the reasons for making the changes.\n",
    "\n",
    "Taking into consideration the tasks required in the machine learning section, use appropriate data cleaning, engineering, extraction and/or other techniques to structure and enrich your data. \n",
    "\n",
    "Rationalize your decisions and implementation, including evidence of how your process has addressed the problems identified in the EDA (Exploratory Data Analysis) stage and how your structured data will assist in the analysis stage. \n",
    "\n",
    "This should include visualizations to illustrate your work and evidence to support your methodology.[0-30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Modeling\n",
    "\n",
    "Clustering, Linear regression, random forest.\n",
    "\n",
    "Use of multiple models (at least two) to compare and contrast results and insights gained.\n",
    "\n",
    "Describe the rationale and justification for the choice of machine learning models for the above-mentioned scenario. \n",
    "\n",
    "Machine Learning models can be used for Prediction, Classification, Clustering, sentiment analysis, recommendation systems and Time series analysis. \n",
    "\n",
    "You should plan on trying multiple approaches (at least two) with proper selection of hyperparameters using GridSearchCV method. \n",
    "\n",
    "You can choose appropriate features from the datasets and a target feature to answer the question asked in the scenario in the case of supervised learning.\n",
    "[0 - 30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aad43947c3feb98b56fcd3385cbe02d1e0f8132ad34b17788f3d25e00be85006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
