{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Setup\n",
    "1. Install and import neccessary packages \n",
    "2. Create functions needed in the report\n",
    "3. Ensure folder structure is properly set up for the project\n",
    "\n",
    "* The project must be explored programmatically, this means that you must implement suitable Python tools (code and/or libraries) to complete the analysis required. All of this is to be implemented in a Jupyter Notebook.[0-50]\n",
    "* The project documentation must include sound justifications and explanation of your code choices. (code quality standards should also be applied) [0-50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" %pip install pandas \n",
    "%pip install numpy  \n",
    "%pip install ipywidgets  \n",
    "%pip install matplotlib \n",
    "%pip install seaborn  \n",
    "%pip install sklearn \n",
    "%pip install pyarrow \"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request as rq\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa # this is needed for the parquet file\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import interact, interactive, fixed, VBox\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE CHANGED\n",
    "# Function to load the Dublin bus gz files\n",
    "def load_Files(direc, files, comtype):\n",
    "    columns = ['Timestamp', 'LineID', 'Direction', 'JourneyPatternID', 'TimeFrame', 'VehicleJourneyID', 'Operator', 'Congestion', 'LonWGS84', 'LatWGS84', 'Delay', 'BlockID', 'VehicleID', 'StopID', 'AtStop']\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        yield pd.read_csv(direc + f, compression=comtype, delimiter=',', header=0, names=columns, parse_dates=True, low_memory=True)\n",
    "\n",
    "def crackit_open(busFile):\n",
    "    import zipfile as zip\n",
    "    # Zip creates its own folders - no need to check for folder existence\n",
    "    with zip.ZipFile(busFile,  mode='r') as arc: \n",
    "        arc.extractall('./Data/Bus/Gz/')  \n",
    "    files = os.listdir('./Data/Bus/Gz/')\n",
    "    DBfiles = [f for f in files if f.endswith('.gz')]\n",
    "    df = pd.concat(load_Files('./Data/Bus/Gz/', DBfiles, 'gzip'), copy = False)\n",
    "    return df\n",
    "\n",
    "def shapiro_test(x):\n",
    "    p_val = stats.shapiro(x)[1]\n",
    "    status = 'passed'\n",
    "    color = 'blue'\n",
    "    if p_val < 0.05:\n",
    "        status = 'failed'\n",
    "        color = 'red'\n",
    "    return status, color, p_val\n",
    "\n",
    "## TO BE CHANGED\n",
    "def custom_scatterplot(df1, col1=''):\n",
    "    df1 = df1[df1[\"LineID\"]==col1]\n",
    "    f = plt.figure()\n",
    "    f, ax = plt.subplots(figsize=(11.5, 11.5))\n",
    "    ax = f.add_subplot(projection='3d')\n",
    "    ax.scatter(df1['LonWGS84'], df1['LatWGS84'], df1['Hour'], alpha=0.6, color=df1['Colour'])\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_zlabel('Hour')\n",
    "    dcol = str(col1)\n",
    "    plt.savefig('./Images/Img_' + dcol + '_Longitude_Latitude_Hour.svg')\n",
    "    df1.to_parquet('./Data/Bus/LineID_' + dcol + '.parquet')\n",
    "    \n",
    "    \n",
    "def custom_barplot(df1, col1=''):\n",
    "    if len(df1[col1]) > 5000: # added this to the function because of warnings about the size of data being used with shapiro test\n",
    "            sampleSize = 5000\n",
    "    else:\n",
    "        sampleSize = len(df1[col1])\n",
    "    df1 = df1.sample(sampleSize) #shapiro test is unreliable over 5000 https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test and performance reasons\n",
    "    f, ax = plt.subplots(2,2, figsize=(11.5, 11.5))\n",
    "    ax = ax.reshape(-1)\n",
    "    df1[col1].plot(ax=ax[0], kind='hist')\n",
    "    ax[0].set_title('Histogram of {}'.format(col1))\n",
    "    df1[col1].plot(ax=ax[1], kind='kde')\n",
    "    ax[1].set_title('Density Plot of {}'.format(col1))\n",
    "    ax3 = plt.subplot(223)\n",
    "    stats.probplot(df[col1], plot=plt)\n",
    "    ax[2].set_title('QQ Plot of {}'.format(col1))\n",
    "    df1[col1].plot(ax=ax[3], kind='box')\n",
    "    ax[3].set_title('Box Plot of {}'.format(col1))\n",
    "    status, color, p_val = shapiro_test(df1[col1]) \n",
    "    f.suptitle('Normality test for {} {} (p_value = {})'.format(col1, status, p_val), color=color, fontsize=12)\n",
    "\n",
    "def num_missing(x):\n",
    "    return len(x.index)-x.count()\n",
    "\n",
    "def num_unique(x):\n",
    "    return len(np.unique(x))\n",
    "\n",
    "## TO BE CHANGED \n",
    "def load_csv_Files(direc, files):\n",
    "\n",
    "    for f in files:\n",
    "        # need to get number of rows to skip \n",
    "        temp=pd.read_csv(direc + f,sep='^',header=None,prefix='X')\n",
    "        temp2=temp.X0.str.split(',',expand=True)\n",
    "        del temp['X0']\n",
    "        temp=pd.concat([temp,temp2],axis=1)\n",
    "        cols = list(range(0,temp.shape[1]))\n",
    "\n",
    "        print(f)\n",
    "        yield pd.read_csv(direc + f,  delimiter=',', header=0,  parse_dates=True, low_memory=True, skiprows=14, usecols=cols, na_values='NAN')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure that the folder structure is present in the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./Data'):\n",
    "    print('Data folder exists')\n",
    "else:\n",
    "    os.makedirs('./Data/Raw/')\n",
    "    os.makedirs('./Data/Final/')\n",
    "if os.path.exists('./Images/'):\n",
    "    print('Images folder exists')\n",
    "else:\n",
    "    os.makedirs('./Images/')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report TITLE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Can python read PDF for sentiment anaylysis?\n",
    "  + the answer to this is that yes Python has various packages that can extract text from pdf files, but none are particularly good or easy to use.\n",
    "+ Are free range pigs more profitable than indoor pigs?\n",
    "+ How does silage quality affect animal prices?\n",
    "  +  Further to the above how does the price of animal feeds affect the price of animals? \n",
    "    + but how do I incorporate sentiment analysis in to this question?\n",
    "+ Does dispossable income of households influence food prices?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP-DM Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Business Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial look at the data, check for missing data\n",
    "Discuss in detail the process of acquiring your raw data, detailing the positive and/or negative aspects of your research and acquisition. This should include the relevance and implications of any and all licensing/permissions associated with the data. [0-15]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory data analysis\n",
    "Exploratory Data Analysis helps to identify patterns, inconsistencies, anomalies, missing data, and other attributes and issues in data sets so problems can be addressed. \n",
    "\n",
    "Evaluate your raw data and detail, in depth, the various attributes and issues that you find. Your evaluation should reference evidence to support your  chosen methodology and use visualizations to illustrate your findings.[0-25]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the shape and types of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the numerical data \n",
    "\n",
    "Use descriptive statistics and appropriate visualisations in order to summarise the dataset(s) used, and to help justify the chosen models. [0-20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Numerical and Categorical columns\n",
    "\n",
    "Analyse the variables in your dataset(s) and use appropriate inferential statistics to gain insights on possible population values \n",
    "\n",
    "(e.g., if you were working with international commerce, you could find a confidence interval for the population proportion of yearly dairy exports out of all agricultural exports). [0-20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display plots from the Data set, including Histograms, pairplots, boxplots and correlation\n",
    "* possibly use seaborn reg plots to check linear regression from the correlation plots \n",
    "\n",
    "Undertake research to find similarities between some country(s) against Ireland, and apply parametric and non-parametric inferential statistical techniques to compare them \n",
    "\n",
    "(e.g., t-test, analysis of variance, Wilcoxon test, chi-squared test, among others). \n",
    "\n",
    "You must justify your choices and verify the applicability of the tests. Hypotheses and conclusions must be clearly stated. \n",
    "\n",
    "You are expected to use at least 5 different inferential statistics tests. [0-40]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Outliers\n",
    "\n",
    "Use the outcome of your analysis to deepen your research. \n",
    "\n",
    "Indicate the challenges you faced in the process. [0-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distributions of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Preparation\n",
    "\n",
    "All data frame changes go in this section, including the reasons for making the changes.\n",
    "\n",
    "Taking into consideration the tasks required in the machine learning section, use appropriate data cleaning, engineering, extraction and/or other techniques to structure and enrich your data. \n",
    "\n",
    "Rationalize your decisions and implementation, including evidence of how your process has addressed the problems identified in the EDA (Exploratory Data Analysis) stage and how your structured data will assist in the analysis stage. \n",
    "\n",
    "This should include visualizations to illustrate your work and evidence to support your methodology.[0-30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect and develop a dataset based on the agriculture topic related to Ireland as well as other parts of the world. \n",
    "\n",
    "Perform a sentimental analysis for an appropriate agricultural topic (e.g., product price, feed quality etc…) for producers and consumers point of view in Ireland.\n",
    "[0 - 25]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Modeling\n",
    "\n",
    "Clustering, Linear regression, random forest.\n",
    "\n",
    "Use of multiple models (at least two) to compare and contrast results and insights gained.\n",
    "\n",
    "Describe the rationale and justification for the choice of machine learning models for the above-mentioned scenario. \n",
    "\n",
    "Machine Learning models can be used for Prediction, Classification, Clustering, sentiment analysis, recommendation systems and Time series analysis. \n",
    "\n",
    "You should plan on trying multiple approaches (at least two) with proper selection of hyperparameters using GridSearchCV method. \n",
    "\n",
    "You can choose appropriate features from the datasets and a target feature to answer the question asked in the scenario in the case of supervised learning.\n",
    "[0 - 30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should train and test for Supervised Learning and other appropriate metrics for unsupervised/ semi-supervised machine learning models that you have chosen. \n",
    "\n",
    "Use cross validation to provide authenticity of the modelling outcomes. \n",
    "\n",
    "You can apply dimensionality reduction methods to prepare the dataset based on your machine learning modelling requirements.\n",
    "[0 - 30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Table or graphics should be provided to illustrate the similarities and contrast of the Machine Learning modelling outcomes based on the scoring metric used for the analysis of the above-mentioned scenario. \n",
    "\n",
    "Discuss and elaborate your understanding clearly.\n",
    "[0 - 15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "Additional notes : \n",
    "All:\n",
    "* Your documentation should present your approach to the project, including elements of project planning ( timelines). \n",
    "* Ensure that your documentation follows a logical sequence through the planning / research / justification / implementation phases of the project. \n",
    "* Ensure that your final upload contains a maximum of 1 jupyter notebook per module.\n",
    "* Please ensure that additional resources are placed and linked to a logical file structure eg, Scripts, Images, Report, Data etc…\n",
    "* Ensure that you include your raw and structured datasets in your submission\n",
    "* 3000(+/- 10%) words in report (not including code, code comments, titles, references or citations) \n",
    "* Your Word count MUST be included\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aad43947c3feb98b56fcd3385cbe02d1e0f8132ad34b17788f3d25e00be85006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
