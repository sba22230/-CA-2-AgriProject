{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install snscrape# Project Setup\n",
    "1. Install and import neccessary packages \n",
    "2. Create functions needed in the report\n",
    "3. Ensure folder structure is properly set up for the project\n",
    "\n",
    "* The project must be explored programmatically, this means that you must implement suitable Python tools (code and/or libraries) to complete the analysis required. All of this is to be implemented in a Jupyter Notebook.[0-50]\n",
    "* The project documentation must include sound justifications and explanation of your code choices. (code quality standards should also be applied) [0-50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.0-py3-none-any.whl (232 kB)\n",
      "     -------------------------------------- 232.4/232.4 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\stehayes\\appdata\\roaming\\python\\python39\\site-packages (from PyPDF2) (4.4.0)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" %pip install pandas \n",
    "%pip install numpy  \n",
    "%pip install ipywidgets  \n",
    "%pip install matplotlib \n",
    "%pip install seaborn  \n",
    "%pip install sklearn \n",
    "%pip install pyarrow \n",
    "# for web scraping\n",
    "%pip install beautifuklsoup4 \n",
    "# for web scraping \n",
    "%pip install requests \"\"\"\n",
    " # for pdf scraping\n",
    "%pip install PyPDF2    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request as rq\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa # this is needed for the parquet file\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import interact, interactive, fixed, VBox\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from bs4 import BeautifulSoup # for web scraping\n",
    "import requests # for web scraping\n",
    "import PyPDF2 # for pdf scraping\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the various search page on the IFA site\n",
    "def searchIFA(searchTerm, pageNumber, header):\n",
    "    \n",
    "    if pageNumber == 1:\n",
    "        url = 'https://www.ifa.ie/?s=' + searchTerm\n",
    "    else:\n",
    "        url = 'https://www.ifa.ie/page/' + str(pageNumber) + '/?s=' + searchTerm \n",
    "    r =requests.get(url, headers = header)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    return soup\n",
    "\n",
    "\n",
    "# Function to find the links in the search results\n",
    "def load_links(soup):\n",
    "    #print(soup.prettify()) # output the html to a file to see what the structure is like determine that the class is \"\"col-sm-12 offset-md-2 col-md-10 col-lg-8 offset-lg-2 col-lg-8\"\" \n",
    "    # not needed in the final code, but useful for debugging\n",
    "    Links = {}\n",
    "    search_Results = soup.find_all(\"div\", {\"class\": \"col-sm-12 offset-md-2 col-md-10 col-lg-8 offset-lg-2 col-lg-8\"})\n",
    "    type(search_Results) # bs4.element.ResultSet is actually a list of the elements that match the search criteria\n",
    "    # loop through the pages and extract the links to the articles\n",
    "    for element in search_Results:\n",
    "        destFilename = element.find('h3').get_text()\n",
    "        # tidy the filename and remove / chatracers\n",
    "        destFilename = destFilename.replace('/', '_')\n",
    "        url = element.find('a').get('href')\n",
    "        Links[destFilename] = url\n",
    "    return Links\n",
    "\n",
    "# Function to save the html pages as text files    \n",
    "def resultText(url, destFilename, header):   # save the html pages as text files\n",
    "    r =requests.get(url, headers = header)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    pageContent = soup.find_all(\"div\", {\"class\": \"single-content\"})\n",
    "    pageTitle = soup.find('h1', {\"class\": \"entry-title\"}).get_text()\n",
    "    for element in pageContent:\n",
    "        text = element.get_text()\n",
    "        txtFile = open('./Data/Raw/' + destFilename + '.txt', 'a', encoding=\"utf-8\")\n",
    "        txtFile.writelines(text)\n",
    "\n",
    "def shapiro_test(x):\n",
    "    p_val = stats.shapiro(x)[1]\n",
    "    status = 'passed'\n",
    "    color = 'blue'\n",
    "    if p_val < 0.05:\n",
    "        status = 'failed'\n",
    "        color = 'red'\n",
    "    return status, color, p_val\n",
    "\n",
    "## TO BE CHANGED\n",
    "def custom_scatterplot(df1, col1=''):\n",
    "    df1 = df1[df1[\"LineID\"]==col1]\n",
    "    f = plt.figure()\n",
    "    f, ax = plt.subplots(figsize=(11.5, 11.5))\n",
    "    ax = f.add_subplot(projection='3d')\n",
    "    ax.scatter(df1['LonWGS84'], df1['LatWGS84'], df1['Hour'], alpha=0.6, color=df1['Colour'])\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_zlabel('Hour')\n",
    "    dcol = str(col1)\n",
    "    plt.savefig('./Images/Img_' + dcol + '_Longitude_Latitude_Hour.svg')\n",
    "    df1.to_parquet('./Data/Bus/LineID_' + dcol + '.parquet')\n",
    "    \n",
    "    \n",
    "def custom_barplot(df1, col1=''):\n",
    "    if len(df1[col1]) > 5000: # added this to the function because of warnings about the size of data being used with shapiro test\n",
    "            sampleSize = 5000\n",
    "    else:\n",
    "        sampleSize = len(df1[col1])\n",
    "    df1 = df1.sample(sampleSize) #shapiro test is unreliable over 5000 https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test and performance reasons\n",
    "    f, ax = plt.subplots(2,2, figsize=(11.5, 11.5))\n",
    "    ax = ax.reshape(-1)\n",
    "    df1[col1].plot(ax=ax[0], kind='hist')\n",
    "    ax[0].set_title('Histogram of {}'.format(col1))\n",
    "    df1[col1].plot(ax=ax[1], kind='kde')\n",
    "    ax[1].set_title('Density Plot of {}'.format(col1))\n",
    "    ax3 = plt.subplot(223)\n",
    "    stats.probplot(df1[col1], plot=plt)\n",
    "    ax[2].set_title('QQ Plot of {}'.format(col1))\n",
    "    df1[col1].plot(ax=ax[3], kind='box')\n",
    "    ax[3].set_title('Box Plot of {}'.format(col1))\n",
    "    status, color, p_val = shapiro_test(df1[col1]) \n",
    "    f.suptitle('Normality test for {} {} (p_value = {})'.format(col1, status, p_val), color=color, fontsize=12)\n",
    "\n",
    "def num_missing(x):\n",
    "    return len(x.index)-x.count()\n",
    "\n",
    "def num_unique(x):\n",
    "    return len(np.unique(x))\n",
    "\n",
    "## TO BE CHANGED \n",
    "def load_csv_Files(direc, files):\n",
    "\n",
    "    for f in files:\n",
    "        # need to get number of rows to skip \n",
    "        temp=pd.read_csv(direc + f,sep='^',header=None,prefix='X')\n",
    "        temp2=temp.X0.str.split(',',expand=True)\n",
    "        del temp['X0']\n",
    "        temp=pd.concat([temp,temp2],axis=1)\n",
    "        cols = list(range(0,temp.shape[1]))\n",
    "\n",
    "        print(f)\n",
    "        yield pd.read_csv(direc + f,  delimiter=',', header=0,  parse_dates=True, low_memory=True, skiprows=14, usecols=cols, na_values='NAN')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure that the folder structure is present in the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./Data'):\n",
    "    print('Data folder exists')\n",
    "else:\n",
    "    os.makedirs('./Data/Raw/')\n",
    "    os.makedirs('./Data/Final/')\n",
    "if os.path.exists('./Images/'):\n",
    "    print('Images folder exists')\n",
    "else:\n",
    "    os.makedirs('./Images/')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the Relationship Between Silage Price, Quality, and the Price of Animals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Can python read PDF for sentiment anaylysis?\n",
    "  + the answer to this is that yes Python has various packages that can extract text from pdf files, but none are particularly good or easy to use.\n",
    "+ Are free range pigs more profitable than indoor pigs?\n",
    "+ How does silage quality affect animal prices?\n",
    "  +  Further to the above how does the price of animal feeds affect the price of animals? \n",
    "    + but how do I incorporate sentiment analysis in to this question?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to examine the relationship between silage price and the price of animals, both as live animals and as a food product. The project will also examine the relationship between the quality of silage and the price of animals, and determine if there is a relationship between the quality of silage and the price of animal feed.\n",
    "\n",
    "Silage is a type of feed that is made by fermenting and preserving green forage crops, such as grass or maize, in a manner that allows it to be stored and fed to livestock over a long period of time. The quality of silage can be affected by several factors, including the type of forage that is used, the method of fermentation and preservation, and the storage conditions.\n",
    "\n",
    "The price of animals, whether as live animals or as a food product, can be affected by a variety of factors, including the demand for the animals and the supply of the animals. The quality of the feed that the animals are given can also affect their health and growth, which in turn can affect their price. This project will examine historical data from the https://www.irishgrassland.ie/journals/ site to determine if there is a relationship between the quality of silage and the price of animals.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the project\n",
    "# Web scraping from https://proxyway.com/guides/web-scraping-with-python#:~:text=Steps%20to%20Build%20a%20Python%20Web%20Scraper%201,parameters.%20...%203%20Step%203%3A%20Write%20the%20Script\n",
    "url = \"https://www.irishgrassland.ie/journals/\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content)\n",
    "soup = soup.find('table')\n",
    "soup = soup.find_all('a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the pdfs from Irish Grassland Journal and save them in the Raw folder\n",
    "# parse the url for the file name\n",
    "\n",
    "for element in soup:\n",
    "    url = element.get('href')\n",
    "    end = url.rfind('_')\n",
    "    if end > 0 : # set this condition as some of the links do not have the underscore in the file name. And these pdfs do not seem to be relevant to the project\n",
    "        start = end - 4\n",
    "        destFilename = 'Irish Grassland and Animal Production Association Journal' + url[start:end]\n",
    "        rq.urlretrieve(url, './Data/Raw/' + destFilename + '.pdf')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text from the pdfs and save them raw folder\n",
    "# reference https://www.geeksforgeeks.org/working-with-pdf-files-in-python/\n",
    "\n",
    "files = os.listdir('./Data/Raw/')\n",
    "files = filter(lambda f: f.endswith(('.pdf','.PDF')), files)\n",
    "\n",
    "for f in files:\n",
    "    pdfFileObj = open('./Data/Raw/' + f, 'rb')\n",
    "    pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "    num_pages = len(pdfReader.pages)\n",
    "    #pageobj = pdfReader.pages[num_pages + 1]\n",
    "    count = 0\n",
    "    text = \"\"\n",
    "    #text = pageobj.extractText()\n",
    "    txtFilename = './Data/Raw/' + f.replace('.pdf', '.txt')\n",
    "    \n",
    "    while count < num_pages:\n",
    "        pageObj = pdfReader.pages[count]\n",
    "        count +=1\n",
    "        text += pageObj.extract_text()\n",
    "   \n",
    "    txtFile = open(txtFilename, 'a', encoding=\"utf-8\")\n",
    "    txtFile.writelines(text) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# download the various articles that show in this search https://www.ifa.ie/?s=silage\n",
    "# There are 8 pages of results for this search, so I have to loop through the pages and extract the links pages are structured as https://www.ifa.ie/page/2/?s=silage\n",
    "\n",
    "# header code re engineered from https://stackoverflow.com/questions/41946166/requests-get-returns-403-while-the-same-url-works-in-browser\n",
    "# only want to declare once in the code\n",
    "h = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'} # This is chrome, you can set whatever browser you like\n",
    "\n",
    "# intialise search and load first page to get the number of pages\n",
    "soup = searchIFA(searchTerm = 'silage', pageNumber = 1, header = h)\n",
    "\n",
    "# find the number of pages returned by the search\n",
    "nav_results = soup.find_all(\"div\", {\"class\": \"nav-links\"})\n",
    "nav_results = nav_results[0].find_all('a')\n",
    "\n",
    "for element in nav_results:\n",
    "    if element.getText() == 'Next':\n",
    "        break\n",
    "    max_page = element.getText()\n",
    "    max_page = int(max_page)\n",
    "\n",
    "# loop through the pages and extract the links\n",
    "for page in range(1, max_page + 1):\n",
    "    print(page)\n",
    "    soup = searchIFA(searchTerm = 'silage', pageNumber = page, header = h)\n",
    "    searchLinks = load_links(soup = soup)\n",
    "    # now that I have the url, I can use beautiful soup to extract the text from the article\n",
    "    for item in searchLinks:\n",
    "        resultText(url = searchLinks[item], destFilename = item, header=h)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Business Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial look at the data, check for missing data\n",
    "Discuss in detail the process of acquiring your raw data, detailing the positive and/or negative aspects of your research and acquisition. This should include the relevance and implications of any and all licensing/permissions associated with the data. [0-15]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory data analysis\n",
    "Exploratory Data Analysis helps to identify patterns, inconsistencies, anomalies, missing data, and other attributes and issues in data sets so problems can be addressed. \n",
    "\n",
    "Evaluate your raw data and detail, in depth, the various attributes and issues that you find. Your evaluation should reference evidence to support your  chosen methodology and use visualizations to illustrate your findings.[0-25]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the shape and types of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the numerical data \n",
    "\n",
    "Use descriptive statistics and appropriate visualisations in order to summarise the dataset(s) used, and to help justify the chosen models. [0-20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Numerical and Categorical columns\n",
    "\n",
    "Analyse the variables in your dataset(s) and use appropriate inferential statistics to gain insights on possible population values \n",
    "\n",
    "(e.g., if you were working with international commerce, you could find a confidence interval for the population proportion of yearly dairy exports out of all agricultural exports). [0-20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display plots from the Data set, including Histograms, pairplots, boxplots and correlation\n",
    "* possibly use seaborn reg plots to check linear regression from the correlation plots \n",
    "\n",
    "Undertake research to find similarities between some country(s) against Ireland, and apply parametric and non-parametric inferential statistical techniques to compare them \n",
    "\n",
    "(e.g., t-test, analysis of variance, Wilcoxon test, chi-squared test, among others). \n",
    "\n",
    "You must justify your choices and verify the applicability of the tests. Hypotheses and conclusions must be clearly stated. \n",
    "\n",
    "You are expected to use at least 5 different inferential statistics tests. [0-40]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Outliers\n",
    "\n",
    "Use the outcome of your analysis to deepen your research. \n",
    "\n",
    "Indicate the challenges you faced in the process. [0-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distributions of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Preparation\n",
    "\n",
    "All data frame changes go in this section, including the reasons for making the changes.\n",
    "\n",
    "Taking into consideration the tasks required in the machine learning section, use appropriate data cleaning, engineering, extraction and/or other techniques to structure and enrich your data. \n",
    "\n",
    "Rationalize your decisions and implementation, including evidence of how your process has addressed the problems identified in the EDA (Exploratory Data Analysis) stage and how your structured data will assist in the analysis stage. \n",
    "\n",
    "This should include visualizations to illustrate your work and evidence to support your methodology.[0-30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect and develop a dataset based on the agriculture topic related to Ireland as well as other parts of the world. \n",
    "\n",
    "Perform a sentimental analysis for an appropriate agricultural topic (e.g., product price, feed quality etc…) for producers and consumers point of view in Ireland.\n",
    "[0 - 25]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Modeling\n",
    "\n",
    "Clustering, Linear regression, random forest.\n",
    "\n",
    "Use of multiple models (at least two) to compare and contrast results and insights gained.\n",
    "\n",
    "Describe the rationale and justification for the choice of machine learning models for the above-mentioned scenario. \n",
    "\n",
    "Machine Learning models can be used for Prediction, Classification, Clustering, sentiment analysis, recommendation systems and Time series analysis. \n",
    "\n",
    "You should plan on trying multiple approaches (at least two) with proper selection of hyperparameters using GridSearchCV method. \n",
    "\n",
    "You can choose appropriate features from the datasets and a target feature to answer the question asked in the scenario in the case of supervised learning.\n",
    "[0 - 30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should train and test for Supervised Learning and other appropriate metrics for unsupervised/ semi-supervised machine learning models that you have chosen. \n",
    "\n",
    "Use cross validation to provide authenticity of the modelling outcomes. \n",
    "\n",
    "You can apply dimensionality reduction methods to prepare the dataset based on your machine learning modelling requirements.\n",
    "[0 - 30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Table or graphics should be provided to illustrate the similarities and contrast of the Machine Learning modelling outcomes based on the scoring metric used for the analysis of the above-mentioned scenario. \n",
    "\n",
    "Discuss and elaborate your understanding clearly.\n",
    "[0 - 15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "Additional notes : \n",
    "All:\n",
    "* Your documentation should present your approach to the project, including elements of project planning ( timelines). \n",
    "* Ensure that your documentation follows a logical sequence through the planning / research / justification / implementation phases of the project. \n",
    "* Ensure that your final upload contains a maximum of 1 jupyter notebook per module.\n",
    "* Please ensure that additional resources are placed and linked to a logical file structure eg, Scripts, Images, Report, Data etc…\n",
    "* Ensure that you include your raw and structured datasets in your submission\n",
    "* 3000(+/- 10%) words in report (not including code, code comments, titles, references or citations) \n",
    "* Your Word count MUST be included\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
